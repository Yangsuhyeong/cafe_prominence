#abline(h = c(qt(1-alpha/2, n - p - 2), -qt(1-alpha/2, n - p - 2)), col = 2)
#####influential point#####
# leverage
h <- diag(H)
sum(h)
#Cook's distance
C <- r^2/(p + 1) * (h / (1 - h))
plot(C, type = "n", main = "Cook's distance")
text(C, cex = 0.7)
abline(h = 1, col = 2, lty = 2)
names(C)[C>1]
infl.id <- names(C)[ C>1 ] #charactor(0)
points(infl.id, C[infl.id], col = 4, cex = 3)
#DFFITS
#2
DFFITS <- t * sqrt(h / (1 - h))
limit <- 2* sqrt((p + 1)/(n - p - 1))
dffit.id <- which(abs(DFFITS) >= limit )
plot(abs(DFFITS), ylim = c(-0.5, 1.5), main = "DFFITS")
abline(h = limit, col = 2, lty = 2)
dffit.id #62,85, 100
#1
DFFITS <- t * sqrt(h / (1 - h))
plot(DFFITS, type="n", ylim = c(-0.5, 1.5), main = "DFFITS")
text(abs(DFFITS), cex= 0.7)
abline(h = 2 * sqrt((p + 1)/(n - p - 1)), col = 2, lty = 2)
#text(abs(DFFITS), cex = 0.7)
#abline(h = 2 * sqrt((p + 1)/(n - p - 1)), col = 2, lty = 2)
#62, 85, 100
#influential point 제거
data3 <- as.data.frame(data[-c(62, 85,100),])
y.1 <- data3$y
x1.1 <- data3$x1; x2.1 <- data3$x2; x3.1 <- data3$x3; x4.1 <- data3$x4; x5.1 <- data3$x5
x6.1 <- data3$x6; x7.1 <- data3$x7; x8.1 <- data3$x8; x9.1 <- data3$x9; x10.1 <- data3$x10
x11.1 <- data3$x11; x12.1 <- data3$x12
e1.1 <- ifelse(x1.1 == 2, 1, 0)
e2.1 <- ifelse(x1.1 == 3, 1, 0)
d2.1 <- x2.1
obj.1 <- lm(y.1 ~ e1.1 + e2.1 + d2.1 + x3.1 + x4.1 + x5.1 + x6.1 + x7.1 + x8.1 + x9.1 + x10.1 + x11.1 + x12.1)
summary(obj.1)
X.1 <- cbind(rep(1,n-3), e1.1, e2.1, d2.1, x3.1, x4.1, x5.1, x6.1, x7.1, x8.1, x9.1, x10.1, x11.1, x12.1)
H.1 <- X.1 %*% solve(t(X.1) %*% X.1) %*% t(X.1)
h.1 <- diag(H.1)
#0.8213
n <- 100
p <- 13
r.1 <- obj.1$residuals
obj.inter2 <- lm(y.1 ~ e1.1 + e2.1 + d2.1 + x3.1 + x4.1 + x5.1 + x6.1 + x7.1 + x8.1 + x9.1 + x10.1 + x11.1 + x12.1
+ e1.1*x3.1 + e1.1*x4.1 + e1.1*x5.1 + e1.1*x6.1 + e1.1*x7.1+ e1.1*x8.1 + e1.1*x9.1 + e1.1*x10.1 +e1.1*x11.1 + e1.1*x12.1
+ e2.1*x3.1 + e2.1*x4.1 + e2.1*x5.1 + e2.1*x6.1 + e2.1*x7.1+ e2.1*x8.1 + e2.1*x9.1 + e2.1*x10.1 +e2.1*x11.1 + e2.1*x12.1
+ d2.1*x3.1 + d2.1*x4.1 + d2.1*x5.1 + d2.1*x6.1 + d2.1*x7.1+ d2.1*x8.1 + d2.1*x9.1 + d2.1*x10.1 +d2.1*x11.1 + d2.1*x12.1)
summary(obj.inter2)
anova(obj.1, obj.inter2)
anova.r2 <- anova(obj.1) #df 86
anova.f2 <- anova(obj.inter2) #df 56
sse.r2 <- anova.r2$`Sum Sq`[14]
sse.f2 <- anova.f2$`Sum Sq`[44]
df.sse.f2 <- obj.inter2$df.residual
F.stat2 <- ((sse.r2 - sse.f2)/30)/(sse.f2/df.sse.f2)
p.value2 <- 1 - pf(F.stat2, 30, df.sse.f2)
print(F.stat2)
print(p.value2)
library(leaps)
new.X1 <- X.1[,-1]
#red.model <- regsubsets(y ~ e1 + e2 + d2+ x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 + x12, data=data, nvmax=p)
#reg.3 <- leaps(x=new.X1, y = y.1, method = c("Cp", "adjr2", "r2", "bic"), nbest=10)
reg.model2 <- regsubsets(y.1 ~ ., as.data.frame(new.X1), nvmax=p)
reg.obj2 <- summary(reg.model2)
reg.obj2$which
reg.obj2$rsq
which.max(reg.obj2$adjr2)
which.min(reg.obj2$cp)
which.min(reg.obj2$bic)
reg.bic2 <- lm(y.1 ~ e1.1 + e2.1 + d2.1 + x3.1 + x10.1 + x12.1)
adjr2.reg2 <- lm(y.1 ~ e1.1 + e2.1 + d2.1 + x3.1 + x10.1 + x12.1)
summary(adjr2.reg2)
summary(reg.bic2)
coef2 <- summary(reg.bic2)$coefficients[,1]
coef2
#만약 dummy 처리 과정에서 e1 <- ifelse(x1 == 1, 1, 0) e2 <- ifelse(x1 == 2, 1, 0)
#로 둔다면, 여기 variable selection 과정에서 e1 과 e2 둘 중 하나만 TRUE가 나오는 경우가 존재한다.
#그럴 때에는, regsubsets 함수의 옵션 중 force.in에서 반드시 모델에 포함되게 만들어 주면 된다.
#influential 제거하지 않은 상태에서 분석하는 과정======================================
#
# obj.inter <- lm(y ~ e1 + e2 + d2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 + x12
#    + e1*x3 + e1*x4 + e1*x5 + e1*x6 + e1*x7+ e1*x8 + e1*x9 + e1*x10 +e1*x11 + e1*x12
#    + e2*x3 + e2*x4 + e2*x5 + e2*x6 + e2*x7+ e2*x8 + e2*x9 + e2*x10 +e2*x11 + e2*x12
#    + d2*x3 + d2*x4 + d2*x5 + d2*x6 + d2*x7+ d2*x8 + d2*x9 + d2*x10 +d2*x11 + d2*x12)
#
# summary(obj.inter)
#
# anova(obj, obj.inter)
#
# anova.r <- anova(obj) #df 86
# anova.f <- anova(obj.inter) #df 56
# sse.r <- anova.r$`Sum Sq`[14]
# sse.f <- anova.f$`Sum Sq`[44]
# df.sse.f <- obj.inter$df.residual
# F.stat <- ((sse.r - sse.f)/30)/(sse.f/df.sse.f)
# p.value <- 1 - pf(F.stat, 30, df.sse.f)
# print(F.stat)
# print(p.value)
# #역시 p-value가 커서 귀무가설을 기각할 수 없다.
#
# #reduced model
# library(leaps)
#
# new.X <- X[,-1]
# #red.model <- regsubsets(y ~ e1 + e2 + d2+ x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 + x12, data=data, nvmax=p)
#
# red.model <- regsubsets(y ~ ., as.data.frame(new.X), nvmax=p)
#
# red.obj <- summary(red.model)
# red.obj$which
# red.obj$rsq
#
# which.max(red.obj$adjr2)
# which.min(red.obj$cp)
# which.min(red.obj$bic)
#
# adjr2.reg <- lm(y ~ e1 + e2 + d2 + x3  + x8 + x10 + x12)
# reg.bic <- lm(y ~ e1 + e2 + d2 + x3 + x10 + x12)
# summary(adjr2.reg)
# summary(reg.bic)
#
# anova(reg.bic, obj)
# #앞의 것이 reduced model, 뒤가 full model. P-value가 매우 크므로 귀무가설인 reg.bic를 기각할 수 없다.
# anova(adjr2.reg, obj)
# #마찬가지로 reduced model인 adjr2.reg를 기각할 수 없다.
# AIC(adjr2.reg, reg.bic)
# BIC(adjr2.reg, reg.bic)
# #adjr2.reg와 reg.bic 두 모델 간의 비교하는 방법 : AIC, BIC.
# #두 방법 모두 reg.bic가 우세했다(값이 작을수록 좋은 모델을 의미).
#
########다른 시도, x를 factor로 묶어 하나로 취급해 버릴 수 없게 하는 경우#########
# data.f <- as.data.frame(data)
# levels(data.f$x1) <- 1:3
# levels(data.f$x2) <- 1:2
#
# library(leaps)
# output <- regsubsets(y ~ data.f$x1 + data.f$x2 +x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 + x12,
#                      data=data.f, method = "exhaustive")
# output
#
# output2 <- regsubsets(y ~ factor(x1) + factor(x2) +x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 + x12,
#                      data=as.data.frame(data), method = "exhaustive")
#
# summary.out <- summary(output2)
# as.data.frame(summary.out$outmat)
#
# summary.out$which
# summary.out$rsq
#
# which.max(summary.out$adjr2)
# which.min(summary.out$cp)
# which.min(red.obj$bic)
#
# adjr2.reg3 <- lm(y~ data.f$x1 + data.f$x2 + x3 + x8 + x10 + x12)
# reg.bic3 <- lm(y ~ data.f$x1 + data.f$x2 + x3 + x10 + x12)
# summary(adjr2.reg3)
# summary(reg.bic3)
#
# anova(reg.bic3, obj)
# anova(adjr2.reg3, obj)
#
# AIC(adjr2.reg3, reg.bic)
# BIC(adjr2.reg3, reg.bic)
#multicolinearity
library(DAAG)
vif.0 <- vif(reg.bic2)
obj.v1 <- lm(e1.1 ~ e2.1 + d2.1 + x3.1 + x10.1 + x12.1, data = data3)
obj.v2 <- lm(e2.1 ~ e1.1 + d2.1 + x3.1 + x10.1 + x12.1, data = data3)
obj.v3 <- lm(d2.1 ~ e1.1 + e2.1 + x3.1 + x10.1 + x12.1, data = data3)
obj.v4 <- lm(x3.1 ~ e1.1 + e2.1 + d2.1 + x10.1 + x12.1, data = data3)
obj.v5 <- lm(x10.1 ~ e1.1 + e2.1 + d2.1 + x3.1 + x12.1, data = data3)
obj.v6 <- lm(x12.1 ~ e1.1 + e2.1 + d2.1 + x3.1 + x10.1, data = data3)
VIF1 <- 1 / (1 - summary(obj.v1)$r.squared)
VIF2 <- 1 / (1 - summary(obj.v2)$r.squared)
VIF3 <- 1 / (1 - summary(obj.v3)$r.squared)
VIF4 <- 1 / (1 - summary(obj.v4)$r.squared)
VIF5 <- 1 / (1 - summary(obj.v5)$r.squared)
VIF6 <- 1 / (1 - summary(obj.v6)$r.squared)
VIF <- cbind(VIF1, VIF2, VIF3, VIF4, VIF5, VIF6)
VIF.0 <- round(as.vector(VIF),4)
#10 이하에서.
#model selection : AIC or BIC => influential 안뺀 모형에서 살펴보았다. influential 뺀것에서는 동일.
#prediction
coef2 <- summary(reg.bic2)$coefficients[ ,1]
e1.0 <- ifelse(test.x[,1]==2, 1, 0)
e2.0 <- ifelse(test.x[,1]==3, 1, 0)
d2.0 <- test.x[,2]
x3.0 <- test.x[,3]; x4.0 <- test.x[,4]; x5.0 <- test.x[,5]
x6.0 <- test.x[,6]; x7.0 <- test.x[,7]; x8.0 <- test.x[,8]
x9.0 <- test.x[,9]; x10.0 <- test.x[,10]
x11.0 <- test.x[,11]; x12.0 <- test.x[,12]
test.bind <- cbind(rep(1, n), e1.0, e2.0, d2.0, x3.0, x10.0, x12.0)
predict <- test.bind %*% coef2
predict.y <- coef2[1]+coef2[2]*e1.0 + coef2[3]*e2.0 + coef2[4]*d2.0 + coef2[5]*x3.0+
coef2[6]*x10.0 + coef2[7]*x12.0
predict.1 <- as.vector(predict)
identical(predict.1, predict.y) #true
plot(y.hat, e, main = "Linear",
xlab = "predicted values",
ylab = "standardized residuals")
abline(h = 0, col = 2, lty = 2)
plot(y.hat, e, main = "Linear",
xlab = "predicted values y hat",
ylab = "standardized residuals")
abline(h = 0, col = 2, lty = 2)
library(lmtest)
dwtest(y ~ e1 + e2 + d2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 + x12)
plot(r)
plot(obj)
e <- r/c(sqrt((1-h)*as.vector(mse)))
qqnorm(e)
qqline(e, col = 2)
temp <- (n - p - 2)/ (n - p - 1 - r^2)
t <- r * sqrt(abs(temp)) * sign(temp)
out.id2 <- which(abs(t) > qt(0.975, n - p - 2)); out.id2
par(mfrow=c(1,1))
plot(t, ylab = "studentized residual", main="studentized residual")
text(out.id2, t[out.id2], out.id2, col=2, cex = 0.9)
abline(h = 0, col = 2, lty = 2)
points(out.id2, t[out.id2], col = 4, cex = 3)
h <- diag(H)
sum(h)
C <- r^2/(p + 1) * (h / (1 - h))
plot(C, type = "n", main = "Cook's distance")
text(C, cex = 0.7)
abline(h = 1, col = 2, lty = 2)
names(C)[C>1]
infl.id <- names(C)[ C>1 ] #charactor(0)
points(infl.id, C[infl.id], col = 4, cex = 3)
DFFITS <- t * sqrt(h / (1 - h))
limit <- 2* sqrt((p + 1)/(n - p - 1))
dffit.id <- which(abs(DFFITS) >= limit )
adjr2.reg2 <- lm(y.1 ~ e1.1 + e2.1 + d2.1 + x3.1 + x10.1 + x12.1)
AIC(adjr2.reg2,reg.bic3)
AIC(adjr2.reg2,reg.bic2)
rm(list=ls())
data <- matrix(scan("C:/Users/양수형/Desktop/data22.txt"), nrow = 100)
test.x <- matrix(scan("C:/Users/양수형/Desktop/prediction22.txt"), nrow = 100)
colnames(data) <- c("y","x1","x2", "x3","x4","x5","x6","x7","x8","x9","x10","x11","x12")
head(data)
data2 <- as.data.frame(data)
colnames(test.x) <- c("x1", "x2", "x3", "x4", "x5", "x6", "x7", "x8", "x9", "x10", "x11", "x12")
head(test.x)
#data frame으로 바꾸어주고 각각 변수불 복사해 사용
y <- data2$y; x1 <- data2$x1; x2 <- data2$x2; x3 <- data2$x3; x4 <- data2$x4
x5 <- data2$x5; x6 <- data2$x6; x7 <- data2$x7; x8 <- data2$x8; x9 <- data2$x9
x10 <- data2$x10; x11 <- data2$x11; x12 <- data2$x12
e1 <- ifelse(x1 == 2, 1, 0)
e2 <- ifelse(x1 == 3, 1, 0)
d2 <- x2-1
#subtract -1 then treated as 0 or 1
obj <- lm(y ~ e1 + e2 + d2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 + x12)
summary(obj)
n <- 100
p <- 13
r <- obj$residuals
#
X <- cbind(rep(1,n), e1, e2, d2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12)
H <- X %*% solve(t(X) %*% X) %*% t(X)
h <- diag(H)
mse <- t(r) %*% r / (n - p - 1)
y.hat <- obj$fitted.values
#scatterplot
par(mfrow = c(2, 7))
for (i in 2:14){
plot(y ~ X[,i], xlab=colnames(X)[i])
abline(lm(y~X[,i]))
}
######diagnosis#########
# studentized residual
#linearity
e <- r/c(sqrt((1-h)*as.vector(mse)))
par(mfrow=c(1,1))
plot(y.hat, e, main = "Linear",
xlab = "predicted values",
ylab = "standardized residuals")
abline(h = 0, col = 2, lty = 2)
#heteroscedasticity
plot(obj)
abline(h = 0, col = 2, lty = 2)
# independence assumption
library(lmtest)
dwtest(y ~ e1 + e2 + d2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 + x12)
plot(r)
#normality
plot(obj)
#outlier
temp <- (n - p - 2)/ (n - p - 1 - r^2)
t <- r * sqrt(abs(temp)) * sign(temp)
out.id2 <- which(abs(t) > qt(0.975, n - p - 2)); out.id2
par(mfrow=c(1,1))
plot(t, ylab = "studentized residual", main="studentized residual")
text(out.id2, t[out.id2], out.id2, col=2, cex = 0.9)
abline(h = 0, col = 2, lty = 2)
points(out.id2, t[out.id2], col = 4, cex = 3)
#plot(t)
#abline(h = c(qt(1-alpha/2, n - p - 2), -qt(1-alpha/2, n - p - 2)), col = 2)
#####influential point#####
# leverage
h <- diag(H)
sum(h)
#Cook's distance
C <- r^2/(p + 1) * (h / (1 - h))
plot(C, type = "n", main = "Cook's distance")
text(C, cex = 0.7)
abline(h = 1, col = 2, lty = 2)
names(C)[C>1]
infl.id <- names(C)[ C>1 ] #charactor(0)
points(infl.id, C[infl.id], col = 4, cex = 3)
#DFFITS
#2
DFFITS <- t * sqrt(h / (1 - h))
limit <- 2* sqrt((p + 1)/(n - p - 1))
dffit.id <- which(abs(DFFITS) >= limit )
plot(abs(DFFITS), ylim = c(-0.5, 1.5), main = "DFFITS")
abline(h = limit, col = 2, lty = 2)
dffit.id #62,85, 100
#1
DFFITS <- t * sqrt(h / (1 - h))
plot(DFFITS, type="n", ylim = c(-0.5, 1.5), main = "DFFITS")
text(abs(DFFITS), cex= 0.7)
abline(h = 2 * sqrt((p + 1)/(n - p - 1)), col = 2, lty = 2)
#text(abs(DFFITS), cex = 0.7)
#abline(h = 2 * sqrt((p + 1)/(n - p - 1)), col = 2, lty = 2)
#62, 85, 100
#influential point 제거
data3 <- as.data.frame(data[-c(62, 85,100),])
y.1 <- data3$y
x1.1 <- data3$x1; x2.1 <- data3$x2; x3.1 <- data3$x3; x4.1 <- data3$x4; x5.1 <- data3$x5
x6.1 <- data3$x6; x7.1 <- data3$x7; x8.1 <- data3$x8; x9.1 <- data3$x9; x10.1 <- data3$x10
x11.1 <- data3$x11; x12.1 <- data3$x12
e1.1 <- ifelse(x1.1 == 2, 1, 0)
e2.1 <- ifelse(x1.1 == 3, 1, 0)
d2.1 <- x2.1
obj.1 <- lm(y.1 ~ e1.1 + e2.1 + d2.1 + x3.1 + x4.1 + x5.1 + x6.1 + x7.1 + x8.1 + x9.1 + x10.1 + x11.1 + x12.1)
summary(obj.1)
X.1 <- cbind(rep(1,n-3), e1.1, e2.1, d2.1, x3.1, x4.1, x5.1, x6.1, x7.1, x8.1, x9.1, x10.1, x11.1, x12.1)
H.1 <- X.1 %*% solve(t(X.1) %*% X.1) %*% t(X.1)
h.1 <- diag(H.1)
#0.8213
n <- 100
p <- 13
r.1 <- obj.1$residuals
obj.inter2 <- lm(y.1 ~ e1.1 + e2.1 + d2.1 + x3.1 + x4.1 + x5.1 + x6.1 + x7.1 + x8.1 + x9.1 + x10.1 + x11.1 + x12.1
+ e1.1*x3.1 + e1.1*x4.1 + e1.1*x5.1 + e1.1*x6.1 + e1.1*x7.1+ e1.1*x8.1 + e1.1*x9.1 + e1.1*x10.1 +e1.1*x11.1 + e1.1*x12.1
+ e2.1*x3.1 + e2.1*x4.1 + e2.1*x5.1 + e2.1*x6.1 + e2.1*x7.1+ e2.1*x8.1 + e2.1*x9.1 + e2.1*x10.1 +e2.1*x11.1 + e2.1*x12.1
+ d2.1*x3.1 + d2.1*x4.1 + d2.1*x5.1 + d2.1*x6.1 + d2.1*x7.1+ d2.1*x8.1 + d2.1*x9.1 + d2.1*x10.1 +d2.1*x11.1 + d2.1*x12.1)
summary(obj.inter2)
anova(obj.1, obj.inter2)
anova.r2 <- anova(obj.1) #df 86
anova.f2 <- anova(obj.inter2) #df 56
sse.r2 <- anova.r2$`Sum Sq`[14]
sse.f2 <- anova.f2$`Sum Sq`[44]
df.sse.f2 <- obj.inter2$df.residual
F.stat2 <- ((sse.r2 - sse.f2)/30)/(sse.f2/df.sse.f2)
p.value2 <- 1 - pf(F.stat2, 30, df.sse.f2)
print(F.stat2)
print(p.value2)
library(leaps)
new.X1 <- X.1[,-1]
#red.model <- regsubsets(y ~ e1 + e2 + d2+ x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 + x12, data=data, nvmax=p)
#reg.3 <- leaps(x=new.X1, y = y.1, method = c("Cp", "adjr2", "r2", "bic"), nbest=10)
reg.model2 <- regsubsets(y.1 ~ ., as.data.frame(new.X1), nvmax=p)
reg.obj2 <- summary(reg.model2)
reg.obj2$which
reg.obj2$rsq
which.max(reg.obj2$adjr2)
which.min(reg.obj2$cp)
which.min(reg.obj2$bic)
reg.bic2 <- lm(y.1 ~ e1.1 + e2.1 + d2.1 + x3.1 + x10.1 + x12.1)
adjr2.reg2 <- lm(y.1 ~ e1.1 + e2.1 + d2.1 + x3.1 + x10.1 + x12.1)
summary(adjr2.reg2)
summary(reg.bic2)
coef2 <- summary(reg.bic2)$coefficients[,1]
coef2
#만약 dummy 처리 과정에서 e1 <- ifelse(x1 == 1, 1, 0) e2 <- ifelse(x1 == 2, 1, 0)
#로 둔다면, 여기 variable selection 과정에서 e1 과 e2 둘 중 하나만 TRUE가 나오는 경우가 존재한다.
#그럴 때에는, regsubsets 함수의 옵션 중 force.in에서 반드시 모델에 포함되게 만들어 주면 된다.
#influential 제거하지 않은 상태에서 분석하는 과정======================================
#
# obj.inter <- lm(y ~ e1 + e2 + d2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 + x12
#    + e1*x3 + e1*x4 + e1*x5 + e1*x6 + e1*x7+ e1*x8 + e1*x9 + e1*x10 +e1*x11 + e1*x12
#    + e2*x3 + e2*x4 + e2*x5 + e2*x6 + e2*x7+ e2*x8 + e2*x9 + e2*x10 +e2*x11 + e2*x12
#    + d2*x3 + d2*x4 + d2*x5 + d2*x6 + d2*x7+ d2*x8 + d2*x9 + d2*x10 +d2*x11 + d2*x12)
#
# summary(obj.inter)
#
# anova(obj, obj.inter)
#
# anova.r <- anova(obj) #df 86
# anova.f <- anova(obj.inter) #df 56
# sse.r <- anova.r$`Sum Sq`[14]
# sse.f <- anova.f$`Sum Sq`[44]
# df.sse.f <- obj.inter$df.residual
# F.stat <- ((sse.r - sse.f)/30)/(sse.f/df.sse.f)
# p.value <- 1 - pf(F.stat, 30, df.sse.f)
# print(F.stat)
# print(p.value)
# #역시 p-value가 커서 귀무가설을 기각할 수 없다.
#
# #reduced model
# library(leaps)
#
# new.X <- X[,-1]
# #red.model <- regsubsets(y ~ e1 + e2 + d2+ x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 + x12, data=data, nvmax=p)
#
# red.model <- regsubsets(y ~ ., as.data.frame(new.X), nvmax=p)
#
# red.obj <- summary(red.model)
# red.obj$which
# red.obj$rsq
#
# which.max(red.obj$adjr2)
# which.min(red.obj$cp)
# which.min(red.obj$bic)
#
# adjr2.reg <- lm(y ~ e1 + e2 + d2 + x3  + x8 + x10 + x12)
# reg.bic <- lm(y ~ e1 + e2 + d2 + x3 + x10 + x12)
# summary(adjr2.reg)
# summary(reg.bic)
#
# anova(reg.bic, obj)
# #앞의 것이 reduced model, 뒤가 full model. P-value가 매우 크므로 귀무가설인 reg.bic를 기각할 수 없다.
# anova(adjr2.reg, obj)
# #마찬가지로 reduced model인 adjr2.reg를 기각할 수 없다.
# AIC(adjr2.reg, reg.bic)
# BIC(adjr2.reg, reg.bic)
# #adjr2.reg와 reg.bic 두 모델 간의 비교하는 방법 : AIC, BIC.
# #두 방법 모두 reg.bic가 우세했다(값이 작을수록 좋은 모델을 의미).
#
########다른 시도, x를 factor로 묶어 하나로 취급해 버릴 수 없게 하는 경우#########
# data.f <- as.data.frame(data)
# levels(data.f$x1) <- 1:3
# levels(data.f$x2) <- 1:2
#
# library(leaps)
# output <- regsubsets(y ~ data.f$x1 + data.f$x2 +x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 + x12,
#                      data=data.f, method = "exhaustive")
# output
#
# output2 <- regsubsets(y ~ factor(x1) + factor(x2) +x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 + x12,
#                      data=as.data.frame(data), method = "exhaustive")
#
# summary.out <- summary(output2)
# as.data.frame(summary.out$outmat)
#
# summary.out$which
# summary.out$rsq
#
# which.max(summary.out$adjr2)
# which.min(summary.out$cp)
# which.min(red.obj$bic)
#
# adjr2.reg3 <- lm(y~ data.f$x1 + data.f$x2 + x3 + x8 + x10 + x12)
# reg.bic3 <- lm(y ~ data.f$x1 + data.f$x2 + x3 + x10 + x12)
# summary(adjr2.reg3)
# summary(reg.bic3)
#
# anova(reg.bic3, obj)
# anova(adjr2.reg3, obj)
#
# AIC(adjr2.reg3, reg.bic)
# BIC(adjr2.reg3, reg.bic)
#multicolinearity
library(DAAG)
vif.0 <- vif(reg.bic2)
obj.v1 <- lm(e1.1 ~ e2.1 + d2.1 + x3.1 + x10.1 + x12.1, data = data3)
obj.v2 <- lm(e2.1 ~ e1.1 + d2.1 + x3.1 + x10.1 + x12.1, data = data3)
obj.v3 <- lm(d2.1 ~ e1.1 + e2.1 + x3.1 + x10.1 + x12.1, data = data3)
obj.v4 <- lm(x3.1 ~ e1.1 + e2.1 + d2.1 + x10.1 + x12.1, data = data3)
obj.v5 <- lm(x10.1 ~ e1.1 + e2.1 + d2.1 + x3.1 + x12.1, data = data3)
obj.v6 <- lm(x12.1 ~ e1.1 + e2.1 + d2.1 + x3.1 + x10.1, data = data3)
VIF1 <- 1 / (1 - summary(obj.v1)$r.squared)
VIF2 <- 1 / (1 - summary(obj.v2)$r.squared)
VIF3 <- 1 / (1 - summary(obj.v3)$r.squared)
VIF4 <- 1 / (1 - summary(obj.v4)$r.squared)
VIF5 <- 1 / (1 - summary(obj.v5)$r.squared)
VIF6 <- 1 / (1 - summary(obj.v6)$r.squared)
VIF <- cbind(VIF1, VIF2, VIF3, VIF4, VIF5, VIF6)
VIF.0 <- round(as.vector(VIF),4)
#10 이하에서.
#model selection : AIC or BIC influential 뺀것에서는 동일.
AIC(adjr2.reg2,reg.bic2) #동일한값 확인
#위에 주석으로 묶은 부분 중 influential point를 빼지 않고 분석한 것이 있는데,
#거기에서는 adjr2와 bic로 뽑힌 모델이 달라 AIC BIC로 수행능력이 나은 모델을 고를 수 있다.
#influential을 뺴면 모두 같은 모델이 도출되어 AIC BIC로 골라줄 필요가 없다.
#prediction
coef2 <- summary(reg.bic2)$coefficients[ ,1]
e1.0 <- ifelse(test.x[,1]==2, 1, 0)
e2.0 <- ifelse(test.x[,1]==3, 1, 0)
d2.0 <- test.x[,2]
x3.0 <- test.x[,3]; x4.0 <- test.x[,4]; x5.0 <- test.x[,5]
x6.0 <- test.x[,6]; x7.0 <- test.x[,7]; x8.0 <- test.x[,8]
x9.0 <- test.x[,9]; x10.0 <- test.x[,10]
x11.0 <- test.x[,11]; x12.0 <- test.x[,12]
test.bind <- cbind(rep(1, n), e1.0, e2.0, d2.0, x3.0, x10.0, x12.0)
predict <- test.bind %*% coef2
predict.y <- coef2[1]+coef2[2]*e1.0 + coef2[3]*e2.0 + coef2[4]*d2.0 + coef2[5]*x3.0+
coef2[6]*x10.0 + coef2[7]*x12.0
predict.1 <- as.vector(predict)
identical(predict.1, predict.y) #true
setwd("C:/Users/양수형/Desktop")
deck <- read.table("final.txt",header=FALSE)
deck2 <- deck[ ,-c(3,5,7,9,11,13,15)]
write.csv(deck2, "final.csv", row.names=FALSE)
View(deck)
View(deck2)
write.csv(deck2, "final3.csv", row.names=FALSE)
